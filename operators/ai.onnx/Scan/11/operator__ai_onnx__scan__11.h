//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__SCAN__11_H
# define OPERATOR_OPERATOR__AI_ONNX__SCAN__11_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'Scan' version 11
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Scan can be used to iterate over one or more scan_input tensors,
 * constructing zero or more scan_output tensors. It combines ideas from general recurrences,
 * functional programming constructs such as scan, fold, map, and zip, and is intended to enable
 * generalizations of RNN-like constructs for sequence-to-sequence processing.
 * Other tensors (referred to as state_variables here) can be used to carry a state
 * when iterating from one element to another (similar to hidden-state in RNNs, also referred
 * to as loop-carried dependences in the context of loops).
 * Many common usages involve a single scan_input tensor (where functionality
 * similar to scan, fold and map can be obtained). When more than one scan_input is used,
 * a behavior similar to zip is obtained.
 * 
 * The attribute body must be a graph, specifying the computation to be performed in
 * every iteration. It takes as input the current values of the state_variables and
 * the current iterated element of the scan_inputs. It must return the (updated) values
 * of the state_variables and zero or more scan_output_element tensors. The values of the
 * scan_output_element tensors are concatenated over all the iterations to produce the
 * scan_output values of the scan construct (similar to the concatenated intermediate
 * hidden-state values of RNN-like constructs). All the output tensors (state_variables as
 * well as scan_output_element tensors) are required to have the same shape in each iteration
 * of the loop (a restriction imposed to enable efficient memory allocation).
 * 
 * Note that the iterated element passed to the body subgraph does not have a sequence
 * axis. It will have a rank one less than the rank of the corresponding scan_input.
 * 
 * The scan operation returns the final values of the state_variables as well as the
 * scan_outputs.
 * 
 * The optional attribute scan_input_directions specifies the direction (forward or backward)
 * for each scan input. If this attribute is omitted, all sequences are scanned in the forward
 * direction. A bidirectional scan may be performed by specifying the same tensor input twice
 * in the scan_inputs, once with a forward direction, and once with a backward direction.
 * 
 * The scan_output of the operation is produced by concatenating the scan_output_element
 * values produced by the body in each iteration.  The optional attribute scan_output_directions
 * specifies the direction in which scan_output is constructed (by appending or prepending the
 * scan_output_element to scan_output in each iteration) for each scan_output. If this attribute
 * is omitted, the scan_output_element is appended to the scan_output in each iteration.
 * 
 * The optional attribute scan_input_axes specifies the axis to be scanned for each scan_input.
 * If omitted, every scan_input will be scanned in axis 0. For example, if axis 0 is the
 * batch axis and axis 1 is the time axis (to be scanned), specify an axis value of 1.
 * Note that scanning a non-zero axis may be less efficient than scanning axis zero.
 * 
 * The optional attribute scan_output_axes specifies the axis along which the scan_outputs
 * are accumulated for each scan_output. For example, if axis 1 is the time axis (to be
 * scanned) for both inputs and outputs, specify a scan_input axis and scan_output axis
 * value of 1.
 * 
 * Note that because of the ONNX restriction that only the last parameter of an operator can
 * be variadic, the initial-states and scan-inputs are listed together as one input parameter.
 * Similarly, the final-states and scan-outputs are listed together as one output parameter.
 * The attribute num_scan_inputs indicates the number M of scan-inputs.
 * 
 * The behavior of
 * 
 *     Scan <
 *         num_scan_inputs = m,
 *         body = loop-body,
 *         scan_input_axes = [axis_1, ..., axis_m]
 *     > (init_1, ..., init_n, scan_1, ..., scan_m)
 * 
 * is equivalent to the following pseudo-code:
 * 
 *     // scan_i.shape[axis_i] denotes the (max) sequence-length of scan_i
 *     // scan_i.shape[axis_i] is required to be equal to scan_j.shape[axis_j] for all i,j.
 *     sequence_length = scan_1.shape[axis_1];
 * 
 *     // initialize state-variables
 *     st_1 = init_1; ... st_n = init_n;
 *     // initialize scan-output variables: [] denotes an empty tensor
 *     scan_out_1 = []; ...; scan_out_k = [];
 *     // identify number of iterations:
 * 
 *     // execute loop
 *     for (int t = 0; t < sequence_length; ++t) {
 *         // generate the scan-input elements: the notation T<axis=k>[t] indicates the sub-tensor
 *         // of rank one less than T obtained by indexing T at position t along axis k.
 *         si_1 = scan_1<axis=axis_1>[t];
 *         ... ;
 *         si_m = scan_m<axis=axis_m>[t];
 *         // execute loop-body
 *         st_1, ..., st_n, so_1, ..., so_k = loop-body(st_1, ..., st_n, si_1, ..., si_m)
 *         // accumulate the scan-output elements
 *         scan_out_1 = Concat<axis=0>(scan_out_1, so_1); ... ; scan_out_k = Concat<axis=0>(scan_out_k, so_k);
 *     }
 * 
 *     return st_1, ..., st_n, scan_out_1, ..., scan_out_k;
 * 
 * *Sample usage: Encoding RNN using a Scan*
 * 
 * The following example shows how a simple RNN over an input tensor %X, with weight tensor %Wi,
 * recurrence weight tensor %Ri, bias tensors %Wbi and %Rbi, and initial hidden-state %H_0 can
 * be encoded as a ScanLoop. Note that the loop-body is a nested graph, and it directly computes
 * %Wi, %Ri, %Wbi, and %Rbi (typically constants or initializers in the body graph). If these
 * values are computed in the outer graph, they need to be passed in as extra state_variables.
 * 
 *     graph rnn-encoding {
 *       %H_0 = ...
 *       %X = ...
 *       %Y_h, %Y = Scan[body = <graph rnn-cell-1>, num_scan_inputs=1](%H_0, %X)
 *       return %Y, %Y_h
 *     }
 * 
 *     graph rnn-cell-1 (
 *       %H_tminus1[FLOAT, tensor]
 *       %X_t[FLOAT, tensor]
 *     ) {
 *       %Wi = ...
 *       %Ri = ...
 *       %Wbi = ...
 *       %Rbi = ...
 *       %t1 = X_t * (Wi^T)
 *       %t2 = H_tminus1*(Ri^T)
 *       %t3 = Add(%t1, %t2)
 *       %t4 = Add(%t3, %Wbi)
 *       %t5 = Add(%t4, %Rbi)
 *       %Ht = Tanh(%t5)
 *       %Accumulate = Identity(%Ht)
 *       return %Ht, %Accumulate
 *     }
 * 
 * Constraint V:
 *   All Tensor types
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * Input V initial_state_and_scan_inputs:
 *   Initial values of the loop's N state variables followed by M scan_inputs
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * Output V final_state_and_scan_outputs:
 *   Final values of the loop's N state variables followed by K scan_outputs
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * Attribute GRAPH body :
 *   The graph run each iteration. It has N+M inputs: (loop state
 *   variables..., scan_input_elts...). It has N+K outputs: (loop state
 *   variables..., scan_output_elts...). Each scan_output is created by
 *   concatenating the value of the specified scan_output_elt value at the end
 *   of each iteration of the loop. It is an error if the dimensions of these
 *   values change across loop iterations.
 * 
 * Attribute INT num_scan_inputs :
 *   An attribute specifying the number of scan_inputs M.
 * 
 * Attribute INTS scan_input_axes (optional):
 *   An optional list of M flags. The i-th element of the list specifies the
 *   axis to be scanned (the sequence axis) for the i-th scan_input. If
 *   omitted, 0 will be used as the scan axis for every scan_input. Negative
 *   value for an axis means counting dimensions from the back. Accepted range
 *   is [-r, r-1] where r = rank(input).
 * 
 * Attribute INTS scan_input_directions (optional):
 *   An optional list of M flags. The i-th element of the list specifies the
 *   direction to be scanned for the i-th scan_input tensor: 0 indicates
 *   forward direction and 1 indicates reverse direction. If omitted, all
 *   scan_input tensors will be scanned in the forward direction.
 * 
 * Attribute INTS scan_output_axes (optional):
 *   An optional list of K flags. The i-th element of the list specifies the
 *   axis for the i-th scan_output. The scan outputs are accumulated along the
 *   specified axis. If omitted, 0 will be used as the scan axis for every
 *   scan_output. Negative value for an axis means counting dimensions from the
 *   back. Accepted range is [-r, r-1].
 * 
 * Attribute INTS scan_output_directions (optional):
 *   An optional list of K flags, one for each scan_output. The i-th element
 *   of the list specifies whether the i-th scan_output should be constructed
 *   by appending or prepending a new value in each iteration: 0 indicates
 *   appending and 1 indicates prepending. If omitted, all scan_output tensors
 *   will be produced by appending a value in each iteration.
 *
 * @since version 11
 *
 * @see github/workspace/onnx/defs/controlflow/old.cc:2607
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#Scan
 */

operator_status
prepare_operator__ai_onnx__scan__11(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__scan__11;

typedef struct {
// no attributes
} context_operator__ai_onnx__scan__11;

operator_executer
resolve_operator__ai_onnx__scan__11(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_complex128(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_complex64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_int16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_int8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_string(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_uint16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_uint32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_uint64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__scan__11__V_tensor_uint8(
    node_context *ctx
);

# endif
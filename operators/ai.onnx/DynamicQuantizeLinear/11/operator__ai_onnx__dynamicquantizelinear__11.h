//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__DYNAMICQUANTIZELINEAR__11_H
# define OPERATOR_OPERATOR__AI_ONNX__DYNAMICQUANTIZELINEAR__11_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'DynamicQuantizeLinear' version 11
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * A Function to fuse calculation for Scale, Zero Point and FP32->8Bit conversion of FP32 Input data.
 * Outputs Scale, ZeroPoint and Quantized Input for a given FP32 Input.
 * Scale is calculated as:
 * ```
 * y_scale = (maximum(0, max(x)) - minimum(0, min(x))) / (qmax - qmin)
 * ```
 * 
 * * where qmax and qmin are max and min values for quantization range i.e. [0, 255] in case of uint8
 * * data range is adjusted to include 0.
 * 
 * Zero point is calculated as:
 * ```
 * intermediate_zero_point = qmin - min(x)/y_scale
 * y_zero_point = cast(round(saturate(itermediate_zero_point)))
 * ```
 * 
 * * where qmax and qmin are max and min values for quantization range .i.e [0, 255] in case of uint8
 * * for saturation, it saturates to [0, 255] if it's uint8, or [-127, 127] if it's int8. Right now only uint8 is supported.
 * * rounding to nearest ties to even.
 * 
 * Data quantization formula is:
 * ```
 * y = saturate (round (x / y_scale) + y_zero_point)
 * ```
 * 
 * * for saturation, it saturates to [0, 255] if it's uint8, or [-127, 127] if it's int8. Right now only uint8 is supported.
 * * rounding to nearest ties to even.
 * 
 * Constraint T1:
 *   Constrain 'x' to float tensor.
 *   Allowed Types: tensor_float
 * 
 * Constraint T2:
 *   Constrain 'y_zero_point' and 'y' to 8-bit unsigned integer tensor.
 *   Allowed Types: tensor_uint8
 * Input T1 x:
 *   Input tensor
 *   Allowed Types: tensor_float
 * Output T2 y:
 *   Quantized output tensor
 *   Allowed Types: tensor_uint8
 * 
 * Output tensor(float) y_scale:
 *   Output scale. It's a scalar, which means a per-tensor/layer quantization.
 *   Allowed Types: tensor_float
 * 
 * Output T2 y_zero_point:
 *   Output zero point. It's a scalar, which means a per-tensor/layer
 *   quantization.
 *   Allowed Types: tensor_uint8

 *
 * @since version 11
 *
 * @see github/workspace/onnx/defs/quantization/defs.cc:241
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#DynamicQuantizeLinear
 */

operator_status
prepare_operator__ai_onnx__dynamicquantizelinear__11(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__dynamicquantizelinear__11;

typedef struct {
// no attributes
} context_operator__ai_onnx__dynamicquantizelinear__11;

operator_executer
resolve_operator__ai_onnx__dynamicquantizelinear__11(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__dynamicquantizelinear__11(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__dynamicquantizelinear__11__T1_tensor_float(
    node_context *ctx
);

# endif
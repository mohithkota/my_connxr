//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__QUANTIZELINEAR__21_H
# define OPERATOR_OPERATOR__AI_ONNX__QUANTIZELINEAR__21_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'QuantizeLinear' version 21
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * The linear quantization operator consumes a high-precision tensor, a scale, and a zero point to compute the
 * low-precision/quantized tensor. The scale factor and zero point must have the same shape, determining the quantization
 * granularity. The quantization formula is `y = saturate((x / y_scale) + y_zero_point)`.
 * 
 * Saturation is done according to:
 * - uint16: [0, 65535]
 * - int16: [-32768, 32767]
 * - uint8: [0, 255]
 * - int8: [-128, 127]
 * - uint4: [0, 15]
 * - int4: [-8, 7]
 * 
 * For `(x / y_scale)`, it rounds to the nearest even. Refer to https://en.wikipedia.org/wiki/Rounding for details.
 * 
 * `y_zero_point` and `y` must have the same type. `y_zero_point` is usually not used for quantization to float8 types, but the quantization
 * formula remains the same for consistency, and the type of the attribute `y_zero_point` still determines the quantization type.
 * 
 * There are three supported quantization granularities, determined by the shape of `y_scale`.
 * In all cases, `y_zero_point` must have the same shape as `y_scale`.
 * - Per-tensor (per-layer) quantization: `y_scale` is a scalar.
 * - Per-axis quantization: The scale must be a 1-D tensor, with the length of the quantization axis. For an input shape
 *  `(D0, ..., Di, ..., Dn)` and `axis=i`, `y_scale` is a 1-D tensor of length `Di`.
 * - Blocked quantization: The scale's shape is identical to the input's shape, except for one dimension, in which
 *   blocking is performed. Given `x` shape `(D0, ..., Di, ..., Dn)`, `axis=i`, and block size `B`: `y_scale` shape is
 *   `(D0, ..., ceil(Di/B), ..., Dn)`.
 * 
 * Constraint T1:
 *   The type of the input 'x'.
 *   Allowed Types: tensor_bfloat16, tensor_float, tensor_float16, tensor_int32
 * 
 * Constraint T2:
 *   The type of the input `y_zero_point` and the output `y`.
 *   Allowed Types: tensor_tensor(float8e4m3fn), tensor_tensor(float8e4m3fnuz),
 *                  tensor_tensor(float8e5m2), tensor_tensor(float8e5m2fnuz),
 *                  tensor_int16, tensor_tensor(int4), tensor_int8,
 *                  tensor_uint16, tensor_tensor(uint4), tensor_uint8
 * Input T1 x:
 *   N-D full precision Input tensor to be quantized.
 *   Allowed Types: tensor_bfloat16, tensor_float, tensor_float16, tensor_int32
 * 
 * Input T1 y_scale:
 *   Scale for doing quantization to get `y`. For per-tensor/layer
 *   quantization the scale is a scalar, for per-axis quantization it is a 1-D
 *   Tensor and for blocked quantization it has the same shape as the input,
 *   except for one dimension in which blocking is performed.
 *   Allowed Types: tensor_bfloat16, tensor_float, tensor_float16, tensor_int32
 * 
 * Input T2 y_zero_point:
 *   Zero point for doing quantization to get `y`. Shape must match
 *   `y_scale`.Default is uint8 with zero point of 0 if it's not specified.
 *   Allowed Types: tensor_tensor(float8e4m3fn), tensor_tensor(float8e4m3fnuz),
 *                  tensor_tensor(float8e5m2), tensor_tensor(float8e5m2fnuz),
 *                  tensor_int16, tensor_tensor(int4), tensor_int8,
 *                  tensor_uint16, tensor_tensor(uint4), tensor_uint8
 * Output T2 y:
 *   N-D quantized output tensor. It has same shape as input `x`.
 *   Allowed Types: tensor_tensor(float8e4m3fn), tensor_tensor(float8e4m3fnuz),
 *                  tensor_tensor(float8e5m2), tensor_tensor(float8e5m2fnuz),
 *                  tensor_int16, tensor_tensor(int4), tensor_int8,
 *                  tensor_uint16, tensor_tensor(uint4), tensor_uint8
 * Attribute INT axis (optional):
 *   (Optional) The axis of the dequantizing dimension of the input tensor.
 *   Used only for per-axis and blocked quantization. Negative value means
 *   counting dimensions from the back. Accepted range is `[-r, r-1]` where `r
 *   = rank(input)`. When the rank of the input is 1, per-tensor quantization
 *   is applied, rendering the axis unnecessary in this scenario.
 * 
 * Attribute INT block_size (optional):
 *   (Optional) The size of the quantization block (number of times every
 *   scale is replicated). Used only for blocked quantization. The block size
 *   is a positive integer. Given `x` shape `(D0, ..., Di, ..., Dn)`, `y_scale`
 *   shape `(S0, ... Si, ...Sn)` and `axis=i`, the accepted range is
 *   `[ceil(Di/Si), ceil(Di/(Si-1))-1]`
 * 
 * Attribute INT output_dtype (optional):
 *   (Optional) The output data type. If not supplied, the output data type is
 *   inferred from `y_zero_point` data type (`T2`). If neither `output_dtype`
 *   nor `y_zero_point` are supplied, output data type is uint8. If both
 *   `output_dtype` and `y_zero_point` are specified, `output_dtype` must be
 *   `T2`.
 * 
 * Attribute INT saturate (optional):
 *   The parameter defines how the conversion behaves if an input value is out
 *   of range of the destination type. It only applies for float 8 quantization
 *   (float8e4m3fn, float8e4m3fnuz, float8e5m2, float8e5m2fnuz). It is true by
 *   default. All cases are fully described in two tables inserted in the
 *   operator description.
 *
 * @since version 21
 *
 * @see github/workspace/onnx/defs/quantization/defs.cc:38
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#QuantizeLinear
 */

operator_status
prepare_operator__ai_onnx__quantizelinear__21(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__quantizelinear__21;

typedef struct {
// no attributes
} context_operator__ai_onnx__quantizelinear__21;

operator_executer
resolve_operator__ai_onnx__quantizelinear__21(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_tensor(float8e4m3fn)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_tensor(float8e4m3fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_tensor(float8e5m2)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_tensor(float8e5m2fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_int16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_tensor(int4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_int8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_uint16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_tensor(uint4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_bfloat16__T2_tensor_uint8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_tensor(float8e4m3fn)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_tensor(float8e4m3fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_tensor(float8e5m2)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_tensor(float8e5m2fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_int16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_tensor(int4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_int8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_uint16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_tensor(uint4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float__T2_tensor_uint8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_tensor(float8e4m3fn)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_tensor(float8e4m3fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_tensor(float8e5m2)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_tensor(float8e5m2fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_int16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_tensor(int4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_int8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_uint16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_tensor(uint4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_float16__T2_tensor_uint8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_tensor(float8e4m3fn)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_tensor(float8e4m3fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_tensor(float8e5m2)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_tensor(float8e5m2fnuz)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_int16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_tensor(int4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_int8(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_uint16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_tensor(uint4)(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__quantizelinear__21__T1_tensor_int32__T2_tensor_uint8(
    node_context *ctx
);

# endif
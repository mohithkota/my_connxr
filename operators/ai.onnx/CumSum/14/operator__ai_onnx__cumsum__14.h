//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__CUMSUM__14_H
# define OPERATOR_OPERATOR__AI_ONNX__CUMSUM__14_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'CumSum' version 14
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Performs cumulative sum of the input elements along the given axis.
 * By default, it will do the sum inclusively meaning the first element is copied as is.
 * Through an `exclusive` attribute, this behavior can change to exclude the first element.
 * It can also perform summation in the opposite direction of the axis. For that, set `reverse` attribute to 1.
 * 
 * Example:
 * ```
 * input_x = [1, 2, 3]
 * axis=0
 * output = [1, 3, 6]
 * exclusive=1
 * output = [0, 1, 3]
 * exclusive=0
 * reverse=1
 * output = [6, 5, 3]
 * exclusive=1
 * reverse=1
 * output = [5, 3, 0]
 * ```
 * 
 * Constraint T:
 *   Constrain input and output types to high-precision numeric tensors.
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * 
 * Constraint T2:
 *   axis tensor can be int32 or int64 only
 *   Allowed Types: tensor_int32, tensor_int64
 * Input T x:
 *   An input tensor that is to be processed.
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * 
 * Input T2 axis:
 *   A 0-D tensor. Must be in the range [-rank(x), rank(x)-1]. Negative value
 *   means counting dimensions from the back.
 *   Allowed Types: tensor_int32, tensor_int64
 * Output T y:
 *   Output tensor of the same type as 'x' with cumulative sums of the x's
 *   elements
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * Attribute INT exclusive (optional):
 *   If set to 1 will return exclusive sum in which the top element is not
 *   included. In other terms, if set to 1, the j-th output element would be
 *   the sum of the first (j-1) elements. Otherwise, it would be the sum of the
 *   first j elements.
 * 
 * Attribute INT reverse (optional):
 *   If set to 1 will perform the sums in reverse direction.
 *
 * @since version 14
 *
 * @see github/workspace/onnx/defs/math/defs.cc:2031
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#CumSum
 */

operator_status
prepare_operator__ai_onnx__cumsum__14(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__cumsum__14;

typedef struct {
// no attributes
} context_operator__ai_onnx__cumsum__14;

operator_executer
resolve_operator__ai_onnx__cumsum__14(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_bfloat16__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_bfloat16__T2_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_double__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_double__T2_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_float__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_float__T2_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_float16__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_float16__T2_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_int32__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_int32__T2_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_int64__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_int64__T2_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_uint32__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_uint32__T2_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_uint64__T2_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__cumsum__14__T_tensor_uint64__T2_tensor_int64(
    node_context *ctx
);

# endif
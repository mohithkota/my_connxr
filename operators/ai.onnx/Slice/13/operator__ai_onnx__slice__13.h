//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__SLICE__13_H
# define OPERATOR_OPERATOR__AI_ONNX__SLICE__13_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'Slice' version 13
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Produces a slice of the input tensor along multiple axes. Similar to numpy:
 * https://numpy.org/doc/stable/user/basics.indexing.html?highlight=slice#slicing-and-striding
 * 
 * Slice uses the `starts`, `ends`, `axes` and `steps` inputs to select a sub-tensor
 * of its input `data` tensor.
 * 
 * An effective `starts[i]`, `ends[i]`, and `steps[i]` must be computed for each `i`
 * in `[0, ... r-1]` where `r = rank(input)` as follows:
 * 
 * If `axes` are omitted, they are set to `[0, ..., r-1]`.
 * If `steps` are omitted, they are set to `[1, ..., 1]` of length `len(starts)`
 * 
 * The effective values are initialized as `start[i] = 0`, `ends[i] = dims[i]` where
 * `dims` are the dimensions of `input` and `steps[i] = 1`.
 * 
 * All negative elements of `axes` are made non-negative by adding `r` to them, where
 * `r =rank(input)`.
 * 
 * All negative values in `starts[i]` and `ends[i]` have `dims[axes[i]]` added to them,
 * where `dims` are the dimensions of `input`. Then `start[axes[i]]` is the adjusted
 * `starts[i]` is clamped into the range `[0, dims[axes[i]]]` for positive stepping
 * and `[0, dims[axes[i]]-1]` for negative stepping.
 * 
 * The clamping for the adjusted `ends[i]` depends on the sign of `steps[i]` and must
 * accommodate copying 0 through `dims[axes[i]]` elements, so for positive stepping
 * `ends[axes[i]]` is clamped to `[0, dims[axes[i]]]`, while for negative stepping it
 * is clamped to `[-1, dims[axes[i]]-1]`.
 * 
 * Finally, `steps[axes[i]] = steps[i]`.
 * 
 * For slicing to the end of a dimension with unknown size, it is recommended to pass
 * in `INT_MAX` when slicing forward and 'INT_MIN' when slicing backward.
 * 
 * Example 1:
 * 
 * ```
 * data = [
 *     [1, 2, 3, 4],
 *     [5, 6, 7, 8],
 * ]
 * axes = [0, 1]
 * starts = [1, 0]
 * ends = [2, 3]
 * steps = [1, 2]
 * result = [
 *     [5, 7],
 * ]
 * ```
 * 
 * Example 2:
 * 
 * ```
 * data = [
 *     [1, 2, 3, 4],
 *     [5, 6, 7, 8],
 * ]
 * starts = [0, 1]
 * ends = [-1, 1000]
 * result = [
 *     [2, 3, 4],
 * ]
 * ```
 * 
 * Constraint T:
 *   Constrain input and output types to all tensor types.
 *   Allowed Types: tensor_bfloat16, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8
 * 
 * Constraint Tind:
 *   Constrain indices to integer types
 *   Allowed Types: tensor_int32, tensor_int64
 * Input T data:
 *   Tensor of data to extract slices from.
 *   Allowed Types: tensor_bfloat16, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8
 * 
 * Input Tind starts:
 *   1-D tensor of starting indices of corresponding axis in `axes`
 *   Allowed Types: tensor_int32, tensor_int64
 * 
 * Input Tind ends:
 *   1-D tensor of ending indices (exclusive) of corresponding axis in `axes`
 *   Allowed Types: tensor_int32, tensor_int64
 * 
 * Input Tind axes:
 *   1-D tensor of axes that `starts` and `ends` apply to. Negative value
 *   means counting dimensions from the back. Accepted range is [-r, r-1] where
 *   r = rank(data). Behavior is undefined if an axis is repeated.
 *   Allowed Types: tensor_int32, tensor_int64
 * 
 * Input Tind steps:
 *   1-D tensor of slice step of corresponding axis in `axes`. Negative value
 *   means slicing backward. 'steps' cannot be 0. Defaults to 1s.
 *   Allowed Types: tensor_int32, tensor_int64
 * Output T output:
 *   Sliced data tensor.
 *   Allowed Types: tensor_bfloat16, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8

 *
 * @since version 13
 *
 * @see github/workspace/onnx/defs/tensor/defs.cc:825
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#Slice
 */

operator_status
prepare_operator__ai_onnx__slice__13(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__slice__13;

typedef struct {
// no attributes
} context_operator__ai_onnx__slice__13;

operator_executer
resolve_operator__ai_onnx__slice__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_bfloat16__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_bfloat16__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_bool__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_bool__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_complex128__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_complex128__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_complex64__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_complex64__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_double__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_double__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_float__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_float__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_float16__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_float16__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int16__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int16__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int32__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int32__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int64__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int64__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int8__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_int8__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_string__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_string__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint16__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint16__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint32__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint32__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint64__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint64__Tind_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint8__Tind_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__slice__13__T_tensor_uint8__Tind_tensor_int64(
    node_context *ctx
);

# endif
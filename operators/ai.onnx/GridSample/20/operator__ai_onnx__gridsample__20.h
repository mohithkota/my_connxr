//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__GRIDSAMPLE__20_H
# define OPERATOR_OPERATOR__AI_ONNX__GRIDSAMPLE__20_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'GridSample' version 20
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Given an input `X` and a flow-field `grid`, computes the output `Y` using `X` values and pixel locations from the `grid`.
 * For spatial input `X` with shape (N, C, H, W), the `grid` will have shape (N, H_out, W_out, 2),
 * the output `Y` will have shape (N, C, H_out, W_out). For volumetric input `X` with shape (N, C, D, H, W),
 * the `grid` will have shape (N, D_out, H_out, W_out, 3), the output `Y` will have shape (N, C, D_out, H_out, W_out).
 * More generally, for an input `X` of rank r+2 with shape (N, C, d1, d2, ..., dr),
 * the `grid` will have shape (N, D1_out, D2_out, ..., Dr_out, r), the output `Y` will have shape (N, C, D1_out, D2_out, ..., Dr_out).
 * 
 * The tensor `X` contains values at centers of square pixels (voxels, etc) locations such as (n, c, d1_in, d2_in, ..., dr_in).
 * The (n, d1_out, d2_out, ..., dr_out, :) values from the tensor `grid` are the normalized positions for interpolating the values
 * at the (n, c, d1_out, d2_out, ..., dr_out) locations from the output tensor `Y` using a specified interpolation method (the mode)
 * and a padding mode (for `grid` positions falling outside the 2-dimensional image).
 * 
 * For example, the values in `grid[n, h_out, w_out, :]` are size-2 vectors specifying normalized positions in the 2-dimensional space of `X`.
 * They are used to interpolate output values of `Y[n, c, h_out, w_out]`.
 * 
 * The GridSample operator is often used in doing grid generator and sampler in the
 * [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).
 * See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html).
 * 
 * Constraint T1:
 *   Constrain input `X` and output `Y` types to all tensor types.
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * 
 * Constraint T2:
 *   Constrain grid types to float tensors.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Input T1 X:
 *   Input tensor of rank r+2 that has shape (N, C, D1, D2, ..., Dr), where N
 *   is the batch size, C is the number of channels, D1, D2, ..., Dr are the
 *   spatial dimensions.
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * 
 * Input T2 grid:
 *   Input offset of shape (N, D1_out, D2_out, ..., Dr_out, r), where D1_out,
 *   D2_out, ..., Dr_out are the spatial dimensions of the grid and output, and
 *   r is the number of spatial dimensions. Grid specifies the sampling
 *   locations normalized by the input spatial dimensions. Therefore, it should
 *   have most values in the range of [-1, 1]. If the grid has values outside
 *   the range of [-1, 1], the corresponding outputs will be handled as defined
 *   by padding_mode. Following computer vision convention, the coordinates in
 *   the length-r location vector are listed from the innermost tensor
 *   dimension to the outermost, the opposite of regular tensor indexing.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Output T1 Y:
 *   Output tensor of rank r+2 that has shape (N, C, D1_out, D2_out, ...,
 *   Dr_out) of the sampled values. For integer input types, intermediate
 *   values are computed as floating point and cast to integer at the end.
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * Attribute INT align_corners (optional):
 *   If align_corners=1, the extrema (-1 and 1) are considered as referring to
 *   the center points of the input's corner pixels (voxels, etc.). If
 *   align_corners=0, they are instead considered as referring to the corner
 *   points of the input's corner pixels (voxels, etc.), making the sampling
 *   more resolution agnostic.
 * 
 * Attribute STRING mode (optional):
 *   Three interpolation modes: linear (default), nearest and cubic. The
 *   "linear" mode includes linear and N-linear interpolation modes depending
 *   on the number of spatial dimensions of the input tensor (i.e. linear for 1
 *   spatial dimension, bilinear for 2 spatial dimensions, etc.). The "cubic"
 *   mode also includes N-cubic interpolation modes following the same rules.
 *   The "nearest" mode rounds to the nearest even index when the sampling
 *   point falls halfway between two indices.
 * 
 * Attribute STRING padding_mode (optional):
 *   Support padding modes for outside grid values: `zeros`(default),
 *   `border`, `reflection`. zeros: use 0 for out-of-bound grid locations,
 *   border: use border values for out-of-bound grid locations, reflection: use
 *   values at locations reflected by the border for out-of-bound grid
 *   locations. If index 0 represents the margin pixel, the reflected value at
 *   index -1 will be the same as the value at index 1. For location far away
 *   from the border, it will keep being reflected until becoming in bound. If
 *   pixel location x = -3.5 reflects by border -1 and becomes x' = 1.5, then
 *   reflects by border 1 and becomes x'' = 0.5.
 *
 * @since version 20
 *
 * @see github/workspace/onnx/defs/tensor/old.cc:37
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#GridSample
 */

operator_status
prepare_operator__ai_onnx__gridsample__20(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__gridsample__20;

typedef struct {
// no attributes
} context_operator__ai_onnx__gridsample__20;

operator_executer
resolve_operator__ai_onnx__gridsample__20(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_bool__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_bool__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_bool__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_complex128__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_complex128__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_complex128__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_complex64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_complex64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_complex64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_double__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_double__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_double__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_float__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_float__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_float__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_float16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_float16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_float16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int32__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int32__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int32__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int8__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int8__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_int8__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_string__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_string__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_string__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint32__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint32__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint32__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint8__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint8__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__20__T1_tensor_uint8__T2_tensor_float16(
    node_context *ctx
);

# endif
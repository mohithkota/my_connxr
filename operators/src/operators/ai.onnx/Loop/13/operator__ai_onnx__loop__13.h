//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__LOOP__13_H
# define OPERATOR_OPERATOR__AI_ONNX__LOOP__13_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'Loop' version 13
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Generic Looping construct. This loop has multiple termination conditions:
 * 
 * 1) Trip count. Iteration count specified at runtime. Set by
 *    specifying the input M. Optional. Set to empty string to omit.
 *    Note that a static trip count (specified at graph construction time) can be
 *    specified by passing in a constant node for input M.
 * 2) Loop termination condition. This is an input to the op that determines
 *    whether to run the first iteration and also a loop-carried dependency for
 *    the body graph. The body graph must yield a value for the condition variable,
 *    whether this input is provided or not.
 * 
 * This table summarizes the operating modes of this operator with equivalent
 * C-style code:
 * 
 *     Operator inputs defined as (max_trip_count, condition_var).
 * 
 *     input ("", ""):
 *         for (int i=0; ; ++i) {
 *           cond = ... // Note this value is ignored, but is required in the body
 *         }
 * 
 *     input ("", cond) // Note this is analogous to a while loop
 *         bool cond = ...;
 *         for (int i=0; cond; ++i) {
 *           cond = ...;
 *         }
 * 
 *     input ("", 1) // Note this is analogous to a do-while loop
 *         bool cond = true
 *         for (int i=0; cond; ++i) {
 *           cond = ...;
 *         }
 * 
 *     input (trip_count, "") // Note this is analogous to a for loop
 *         int trip_count = ...
 *         for (int i=0; i < trip_count; ++i) {
 *           cond = ...; // ignored
 *         }
 * 
 *     input (trip_count, cond)
 *         int trip_count = ...;
 *         bool cond = ...;
 *         for (int i=0; i < trip_count && cond; ++i) {
 *           cond = ...;
 *         }
 * 
 * 
 * *Sample usage - cond as well as trip count*
 * 
 *     graph predict-net {
 *       %a = Constant[value = <Scalar Tensor [3]>]()
 *       %b = Constant[value = <Scalar Tensor [6]>]()
 *       %keepgoing = Constant[value = <Scalar Tensor [1]>]()
 *       %max_trip_count = Constant[value = <Scalar Tensor [10]>]()
 *       %keepgoing_out, %b_out, %user_defined_vals = Loop[body = <graph body-net>](%max_trip_count, %keepgoing, %b)
 *       return
 *     }
 * 
 *     graph body-net (
 *       %i[INT32, scalar]           // iteration number
 *       %keepgoing_in[BOOL, scalar] // incoming loop-termination-condition; not used
 *       %b_in[INT32, scalar]        // incoming value of loop-carried-dependency b
 *     ) {
 *       %my_local = Add(%a, %b_in)
 *       %b_out = Sub(%a, %b_in) // outgoing value of loop-carried-dependency b
 *       %keepgoing_out = Greater(%my_local, %b_out) // outgoing loop-termination-condition
 *       %user_defined_val = Add(%b_in, %b_in) // scan-output value to be accumulated
 *       return %keepgoing_out, %b_out, %user_defined_val
 *     }
 * 
 * *Sample equivalent C code*
 * 
 *     {
 *       /* User-defined code (enclosing scope) */
 *       int a = 3, b = 6;
 *       bool keepgoing = true; // Analogous to input cond
 *       /* End user-defined code */
 * 
 *       /* Implicitly-defined code */
 *       const int max_trip_count = 10; // Analogous to input M
 *       int user_defined_vals[]; // Imagine this is resizable
 *       /* End implicitly-defined code */
 *       /* initialize loop-carried variables and scan-output variables */
 *       bool keepgoing_out = keepgoing
 *       int b_out = b
 * 
 *       for (int i=0; i < max_trip_count && keepgoing_out; ++i) {
 *         /* Implicitly-defined code: bind actual parameter values
 *            to formal parameter variables of loop-body */
 *         bool keepgoing_in = keepgoing_out;
 *         bool b_in = b_out;
 * 
 *         /* User-defined code (loop body) */
 *         int my_local = a + b_in; // Reading value "a" from the enclosing scope is fine
 *         b_out = a - b_in;
 *         keepgoing_out = my_local > b_out;
 *         user_defined_val = b_in + b_in; // b_in and b_out are different variables
 *         /* End user-defined code */
 * 
 *         /* Implicitly defined-code */
 *         user_defined_vals[i] = user_defined_val // accumulate scan-output values
 *       }
 *       // int t = my_local; // Can't do this. my_local is not accessible here.
 * 
 *       // The values below are bound to the output variables of the loop and therefore accessible
 *       // b_out; user_defined_vals; keepgoing_out;
 *     }
 * 
 * There are several things of note in this code snippet:
 * 
 * 1) Values from the enclosing scope (i.e. variable "a" here) are in scope and can
 *    be referenced in the inputs of the loop.
 * 2) Any values computed in the loop body that needs to be used in a subsequent
 *    iteration or after the loop are modelled using a pair of variables in the loop-body,
 *    consisting of an input variable (eg., b_in) and an output variable (eg., b_out).
 *    These are referred to as loop-carried dependences. The loop operation node
 *    supplies the input value of the input variable for the first iteration, and
 *    returns the output value of the output variable produced by the final
 *    iteration.
 * 3) Scan_output variables are used to implicitly concatenate values computed across
 *    all the iterations. In the above example, the value of user_defined_val computed
 *    over all iterations are concatenated and returned as the value of user_defined_vals
 *    after the loop.
 * 4) Values created in the body cannot be accessed in the enclosing scope,
 *    except using the mechanism described above.
 * 
 * Note that the semantics of this op support "diagonal" or "wavefront" execution.
 * (See Step 3 here for an example:
 * https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/).
 * Frontends should emit multi-layer RNNs as a series of While operators (with
 * time being the inner looping dimension), with each successive layer consuming
 * the scan_outputs from the previous layer, possibly going through several
 * point-wise operators (e.g. dropout, residual connections, linear layer).
 * 
 * The input/output of subgraph (produced by loop node) matching is based on order instead of name. The implementation will figure out the names based on this order.
 * 
 * Constraint V:
 *   All Tensor and Sequence types
 *   Allowed Types: seq_tensor_bool, seq_tensor_complex128,
 *                  seq_tensor_complex64, seq_tensor_double, seq_tensor_float,
 *                  seq_tensor_float16, seq_tensor_int16, seq_tensor_int32,
 *                  seq_tensor_int64, seq_tensor_int8, seq_tensor_string,
 *                  seq_tensor_uint16, seq_tensor_uint32, seq_tensor_uint64,
 *                  seq_tensor_uint8, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8
 * 
 * Constraint I:
 *   tensor of int64, which should be a scalar.
 *   Allowed Types: tensor_int64
 * 
 * Constraint B:
 *   tensor of bool, which should be a scalar.
 *   Allowed Types: tensor_bool
 * Input I M:
 *   A maximum trip-count for the loop specified at runtime. Optional. Pass
 *   empty string to skip.
 *   Allowed Types: tensor_int64
 * 
 * Input B cond:
 *   A boolean termination condition. Optional. Pass empty string to skip.
 *   Allowed Types: tensor_bool
 * 
 * Input V v_initial:
 *   The initial values of any loop-carried dependencies (values that change
 *   across loop iterations)
 *   Allowed Types: seq_tensor_bool, seq_tensor_complex128,
 *                  seq_tensor_complex64, seq_tensor_double, seq_tensor_float,
 *                  seq_tensor_float16, seq_tensor_int16, seq_tensor_int32,
 *                  seq_tensor_int64, seq_tensor_int8, seq_tensor_string,
 *                  seq_tensor_uint16, seq_tensor_uint32, seq_tensor_uint64,
 *                  seq_tensor_uint8, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8
 * Output V v_final_and_scan_outputs:
 *   Final N loop carried dependency values then K scan_outputs. Scan outputs
 *   must be Tensors.
 *   Allowed Types: seq_tensor_bool, seq_tensor_complex128,
 *                  seq_tensor_complex64, seq_tensor_double, seq_tensor_float,
 *                  seq_tensor_float16, seq_tensor_int16, seq_tensor_int32,
 *                  seq_tensor_int64, seq_tensor_int8, seq_tensor_string,
 *                  seq_tensor_uint16, seq_tensor_uint32, seq_tensor_uint64,
 *                  seq_tensor_uint8, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8
 * Attribute GRAPH body :
 *   The graph run each iteration. It has 2+N inputs: (iteration_num,
 *   condition, loop carried dependencies...). It has 1+N+K outputs:
 *   (condition, loop carried dependencies..., scan_outputs...). Each
 *   scan_output is created by concatenating the value of the specified output
 *   value at the end of each iteration of the loop. It is an error if the
 *   dimensions or data type of these scan_outputs change across loop
 *   iterations.
 *
 * @since version 13
 *
 * @see github/workspace/onnx/defs/controlflow/old.cc:2423
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#Loop
 */

operator_status
prepare_operator__ai_onnx__loop__13(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__loop__13;

typedef struct {
// no attributes
} context_operator__ai_onnx__loop__13;

operator_executer
resolve_operator__ai_onnx__loop__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_bool__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_complex128__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_complex64__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_double__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_float__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_float16__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_int16__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_int32__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_int64__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_int8__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_string__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_uint16__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_uint32__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_uint64__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_seq_tensor_uint8__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_bool__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_complex128__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_complex64__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_double__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_float__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_float16__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_int16__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_int32__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_int64__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_int8__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_string__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_uint16__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_uint32__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_uint64__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__loop__13__V_tensor_uint8__I_tensor_int64__B_tensor_bool(
    node_context *ctx
);

# endif
//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__GRIDSAMPLE__16_H
# define OPERATOR_OPERATOR__AI_ONNX__GRIDSAMPLE__16_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'GridSample' version 16
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Given an input `X` and a flow-field `grid`, computes the output `Y` using `X` values and pixel locations from `grid`.
 * Currently, only spatial (4-D) inputs are supported. For input `X` with shape (N, C, H, W) and `grid` with shape (N, H_out, W_out, 2),
 * the output `Y` will have shape (N, C, H_out, W_out).
 * 
 * The tensor `X` contains values at centers of square pixels in a H by W 2-dimensional image.
 * The tensor `grid` describes normalized positions where the output `Y` is to be computed
 * using a specified interpolation method (the mode) and a padding mode (for grid positions falling outside the 2-dimensional image).
 * 
 * Elements in `grid[N, H_out, W_out]` are size-2 vectors specifying positions in the 2-dimensional space of `X`.
 * They are used to interpolate output values of `Y[N, C, H_out, W_out]`.
 * 
 * The GridSample operator is often used in doing grid generator and sampler in the [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).
 * See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample).
 * 
 * Constraint T1:
 *   Constrain input `X` and output `Y` types to all tensor types.
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * 
 * Constraint T2:
 *   Constrain grid types to float tensors.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Input T1 X:
 *   4-D tensor of shape (N, C, H, W), where N is the batch size, C is the
 *   numbers of channels, H and W are the height and width of the input data.
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * 
 * Input T2 grid:
 *   Input offset, 4-D tensor of shape (N, H_out, W_out, 2), where H_out and
 *   W_out are the height and width of grid and output, Grid specifies the
 *   sampling pixel locations normalized by the input spatial dimensions.
 *   Therefore, it should have most values in the range of [-1, 1]. If grid has
 *   values outside the range of [-1, 1], the corresponding outputs will be
 *   handled as defined by padding_mode.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Output T1 Y:
 *   4-D tensor of shape (N, C, H_out, W_out) of sampled values. For integer
 *   input types, intermediate values are computed as floating point and cast
 *   to integer at the end.
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * Attribute INT align_corners (optional):
 *   If align_corners=1, the extrema (-1 and 1) are considered as referring to
 *   the center points of the input's corner pixels. If align_corners=0, they
 *   are instead considered as referring to the corner points of the input's
 *   corner pixels, making the sampling more resolution agnostic.
 * 
 * Attribute STRING mode (optional):
 *   Three interpolation modes: bilinear (default), nearest and bicubic.
 * 
 * Attribute STRING padding_mode (optional):
 *   Support padding modes for outside grid values: `zeros`(default),
 *   `border`, `reflection`. zeros: use 0 for out-of-bound grid locations,
 *   border: use border values for out-of-bound grid locations, reflection: use
 *   values at locations reflected by the border for out-of-bound grid
 *   locations. If index 0 represents the margin pixel, the reflected value at
 *   index -1 will be the same as the value at index 1. For location far away
 *   from the border, it will keep being reflected until becoming in bound. If
 *   pixel location x = -3.5 reflects by border -1 and becomes x' = 1.5, then
 *   reflects by border 1 and becomes x'' = 0.5.
 *
 * @since version 16
 *
 * @see github/workspace/onnx/defs/tensor/old.cc:645
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#GridSample
 */

operator_status
prepare_operator__ai_onnx__gridsample__16(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__gridsample__16;

typedef struct {
// no attributes
} context_operator__ai_onnx__gridsample__16;

operator_executer
resolve_operator__ai_onnx__gridsample__16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_bool__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_bool__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_bool__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_complex128__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_complex128__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_complex128__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_complex64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_complex64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_complex64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_double__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_double__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_double__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_float__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_float__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_float__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_float16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_float16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_float16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int32__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int32__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int32__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int8__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int8__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_int8__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_string__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_string__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_string__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint32__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint32__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint32__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint8__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint8__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gridsample__16__T1_tensor_uint8__T2_tensor_float16(
    node_context *ctx
);

# endif
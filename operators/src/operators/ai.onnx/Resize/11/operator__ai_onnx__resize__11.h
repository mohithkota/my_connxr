//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__RESIZE__11_H
# define OPERATOR_OPERATOR__AI_ONNX__RESIZE__11_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'Resize' version 11
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Resize the input tensor. In general, it calculates every value in the output tensor as a weighted average of neighborhood (a.k.a. sampling locations) in the input tensor.
 * Each dimension value of the output tensor is:
 *   output_dimension = floor(input_dimension * (roi_end - roi_start) * scale) if input \"sizes\" is not specified.
 * 
 * Constraint T1:
 *   Constrain input 'X' and output 'Y' to all tensor types.
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * 
 * Constraint T2:
 *   Constrain roi type to float or double.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Input T1 X:
 *   N-D tensor
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * 
 * Input T2 roi:
 *   1-D tensor given as [start1, ..., startN, end1, ..., endN], where N is
 *   the rank of X. The RoIs' coordinates are normalized in the coordinate
 *   system of the input image. It only takes effect when
 *   coordinate_transformation_mode is "tf_crop_and_resize"
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * 
 * Input tensor(float) scales:
 *   The scale array along each dimension. It takes value greater than 0. If
 *   it's less than 1, it's sampling down, otherwise, it's upsampling. The
 *   number of elements of 'scales' should be the same as the rank of input
 *   'X'. If 'size' is needed, the user must set 'scales' to an empty tensor.
 *   Allowed Types: tensor_float
 * 
 * Input tensor(int64) sizes:
 *   The size of the output tensor. The number of elements of 'sizes' should
 *   be the same as the rank of input 'X'. May only be set if 'scales' is set
 *   to an empty tensor.
 *   Allowed Types: tensor_int64
 * Output T1 Y:
 *   N-D tensor after resizing
 *   Allowed Types: tensor_bool, tensor_complex128, tensor_complex64,
 *                  tensor_double, tensor_float, tensor_float16, tensor_int16,
 *                  tensor_int32, tensor_int64, tensor_int8, tensor_string,
 *                  tensor_uint16, tensor_uint32, tensor_uint64, tensor_uint8
 * Attribute STRING coordinate_transformation_mode (optional):
 *   This attribute describes how to transform the coordinate in the resized
 *   tensor to the coordinate in the original tensor. <br/> The coordinate of
 *   each dimension is transformed individually. Let's describe a case using
 *   axis x as an example. Denote x_resized as the coordinate of axis x in the
 *   resized tensor, x_original as the coordinate of axis x in the original
 *   tensor, length_original as the length of the original tensor in axis x,
 *   length_resized as the length of the resized tensor in axis x, roi_x =
 *   (start_x, end_x) of the axis x in input "roi", scale = length_resized /
 *   length_original, <br/> if coordinate_transformation_mode is "half_pixel",
 *   <br/> x_original = (x_resized + 0.5) / scale - 0.5, <br/> if
 *   coordinate_transformation_mode is "pytorch_half_pixel", <br/> x_original =
 *   length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/> if
 *   coordinate_transformation_mode is "align_corners", <br/> x_original =
 *   x_resized * (length_original - 1) / (length_resized - 1), <br/> if
 *   coordinate_transformation_mode is "asymmetric", <br/> x_original =
 *   x_resized / scale, <br/> if coordinate_transformation_mode is
 *   "tf_half_pixel_for_nn", <br/> x_original = (x_resized + 0.5) / scale,
 *   <br/> if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
 *   x_original = length_resized > 1 ? start_x * (length_original - 1) +
 *   x_resized * (end_x - start_x) * (length_original - 1) / (length_resized -
 *   1) : 0.5 * (start_x + end_x) * (length_original - 1).
 * 
 * Attribute FLOAT cubic_coeff_a (optional):
 *   The coefficient 'a' used in cubic interpolation. Two common choice are
 *   -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out
 *   Equation (4) in https://ieeexplore.ieee.org/document/1163711 for the
 *   details. This attribute is valid only if "mode" is "cubic".
 * 
 * Attribute INT exclude_outside (optional):
 *   If set to 1, the weight of sampling locations outside the tensor will be
 *   set to 0 and the weight will be renormalized so that their sum is 1.0. The
 *   default value is 0.
 * 
 * Attribute FLOAT extrapolation_value (optional):
 *   When coordinate_transformation_mode is "tf_crop_and_resize" and
 *   x_original is outside the range [0, length_original - 1], this value is
 *   used as the corresponding output value. Default is 0.0f.
 * 
 * Attribute STRING mode (optional):
 *   Three interpolation modes: nearest (default), linear and cubic. The
 *   "linear" mode includes linear interpolation for 1D tensor and N-linear
 *   interpolation for N-D tensor (for example, bilinear interpolation for 2D
 *   tensor). The "cubic" mode includes cubic interpolation for 1D tensor and
 *   N-cubic interpolation for N-D tensor (for example, bicubic interpolation
 *   for 2D tensor).
 * 
 * Attribute STRING nearest_mode (optional):
 *   Four modes: round_prefer_floor (default, as known as round half down),
 *   round_prefer_ceil (as known as round half up), floor, ceil. Only used by
 *   nearest interpolation. It indicates how to get "nearest" pixel in input
 *   tensor from x_original, so this attribute is valid only if "mode" is
 *   "nearest".
 *
 * @since version 11
 *
 * @see github/workspace/onnx/defs/tensor/old.cc:3446
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#Resize
 */

operator_status
prepare_operator__ai_onnx__resize__11(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__resize__11;

typedef struct {
// no attributes
} context_operator__ai_onnx__resize__11;

operator_executer
resolve_operator__ai_onnx__resize__11(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_bool__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_bool__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_bool__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_complex128__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_complex128__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_complex128__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_complex64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_complex64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_complex64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_double__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_double__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_double__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_float__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_float__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_float__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_float16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_float16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_float16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int32__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int32__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int32__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int8__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int8__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_int8__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_string__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_string__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_string__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint16__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint16__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint16__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint32__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint32__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint32__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint64__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint64__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint64__T2_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint8__T2_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint8__T2_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__resize__11__T1_tensor_uint8__T2_tensor_float16(
    node_context *ctx
);

# endif
//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__GEMM__13_H
# define OPERATOR_OPERATOR__AI_ONNX__GEMM__13_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'Gemm' version 13
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * General Matrix multiplication:
 * https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3
 * 
 * * A' = transpose(A) if transA else A
 * * B' = transpose(B) if transB else B
 * 
 * Compute Y = alpha * A' * B' + beta * C, where input tensor A has shape (M, K) or (K, M),
 * input tensor B has shape (K, N) or (N, K), input tensor C is broadcastable to shape (M, N),
 * and output tensor Y has shape (M, N). A will be transposed before doing the
 * computation if attribute transA is non-zero, same for B and transB.
 * This operator supports **unidirectional broadcasting** (tensor C should be unidirectional broadcastable to tensor A * B); for more details please check [the doc](Broadcasting.md).
 * This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
 * 
 * Constraint T:
 *   Constrain input and output types to float/int tensors.
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * Input T A:
 *   Input tensor A. The shape of A should be (M, K) if transA is 0, or (K, M)
 *   if transA is non-zero.
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * 
 * Input T B:
 *   Input tensor B. The shape of B should be (K, N) if transB is 0, or (N, K)
 *   if transB is non-zero.
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * 
 * Input T C:
 *   Optional input tensor C. If not specified, the computation is done as if
 *   C is a scalar 0. The shape of C should be unidirectional broadcastable to
 *   (M, N).
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * Output T Y:
 *   Output tensor of shape (M, N).
 *   Allowed Types: tensor_bfloat16, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int32, tensor_int64, tensor_uint32,
 *                  tensor_uint64
 * Attribute FLOAT alpha (optional):
 *   Scalar multiplier for the product of input tensors A * B.
 * 
 * Attribute FLOAT beta (optional):
 *   Scalar multiplier for input tensor C.
 * 
 * Attribute INT transA (optional):
 *   Whether A should be transposed
 * 
 * Attribute INT transB (optional):
 *   Whether B should be transposed
 *
 * @since version 13
 *
 * @see github/workspace/onnx/defs/math/defs.cc:1272
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#Gemm
 */

operator_status
prepare_operator__ai_onnx__gemm__13(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__gemm__13;

typedef struct {
// no attributes
} context_operator__ai_onnx__gemm__13;

operator_executer
resolve_operator__ai_onnx__gemm__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_bfloat16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_float16(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_uint32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gemm__13__T_tensor_uint64(
    node_context *ctx
);

# endif
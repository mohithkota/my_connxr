//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__TILE__13_H
# define OPERATOR_OPERATOR__AI_ONNX__TILE__13_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'Tile' version 13
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Constructs a tensor by tiling a given tensor.
 * This is the same as function `tile` in Numpy, but no broadcast.
 * For example A = [[1, 2], [3, 4]], B = [1, 2], tile(A, B) = [[1, 2, 1, 2], [3, 4, 3, 4]]
 * 
 * Constraint T:
 *   Constrain input and output types to all tensor types.
 *   Allowed Types: tensor_bfloat16, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8
 * 
 * Constraint T1:
 *   Constrain repeat's type to int64 tensors.
 *   Allowed Types: tensor_int64
 * Input T input:
 *   Input tensor of any shape.
 *   Allowed Types: tensor_bfloat16, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8
 * 
 * Input T1 repeats:
 *   1D int64 tensor of the same length as input's dimension number, includes
 *   numbers of repeated copies along input's dimensions.
 *   Allowed Types: tensor_int64
 * Output T output:
 *   Output tensor of the same dimensions and type as tensor input.
 *   output_dim[i] = input_dim[i] * repeats[i]
 *   Allowed Types: tensor_bfloat16, tensor_bool, tensor_complex128,
 *                  tensor_complex64, tensor_double, tensor_float,
 *                  tensor_float16, tensor_int16, tensor_int32, tensor_int64,
 *                  tensor_int8, tensor_string, tensor_uint16, tensor_uint32,
 *                  tensor_uint64, tensor_uint8

 *
 * @since version 13
 *
 * @see github/workspace/onnx/defs/tensor/defs.cc:2026
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#Tile
 */

operator_status
prepare_operator__ai_onnx__tile__13(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__tile__13;

typedef struct {
// no attributes
} context_operator__ai_onnx__tile__13;

operator_executer
resolve_operator__ai_onnx__tile__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_bfloat16__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_bool__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_complex128__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_complex64__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_double__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_float__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_float16__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_int16__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_int32__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_int64__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_int8__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_string__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_uint16__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_uint32__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_uint64__T1_tensor_int64(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__tile__13__T_tensor_uint8__T1_tensor_int64(
    node_context *ctx
);

# endif
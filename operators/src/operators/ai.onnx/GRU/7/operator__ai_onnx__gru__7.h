//this file was generated by ../../../../../../../../connx/connx_ajit/scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__GRU__7_H
# define OPERATOR_OPERATOR__AI_ONNX__GRU__7_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'GRU' version 7
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * Computes an one-layer GRU. This operator is usually supported via some custom
 * implementation such as CuDNN.
 * 
 * Notations:
 * 
 * `X` - input tensor
 * 
 * `z` - update gate
 * 
 * `r` - reset gate
 * 
 * `h` - hidden gate
 * 
 * `t` - time step (t-1 means previous time step)
 * 
 * `W[zrh]` - W parameter weight matrix for update, reset, and hidden gates
 * 
 * `R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates
 * 
 * `Wb[zrh]` - W bias vectors for update, reset, and hidden gates
 * 
 * `Rb[zrh]` - R bias vectors for update, reset, and hidden gates
 * 
 * `WB[zrh]` - W parameter weight matrix for backward update, reset, and hidden gates
 * 
 * `RB[zrh]` - R recurrence weight matrix for backward update, reset, and hidden gates
 * 
 * `WBb[zrh]` - W bias vectors for backward update, reset, and hidden gates
 * 
 * `RBb[zrh]` - R bias vectors for backward update, reset, and hidden gates
 * 
 * `H` - Hidden state
 * 
 * `num_directions` - 2 if direction == bidirectional else 1
 * 
 * Activation functions:
 * 
 *   Relu(x)                - max(0, x)
 * 
 *   Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
 * 
 *   Sigmoid(x)             - 1/(1 + e^{-x})
 * 
 *   (NOTE: Below are optional)
 * 
 *   Affine(x)              - alpha*x + beta
 * 
 *   LeakyRelu(x)           - x if x >= 0 else alpha * x
 * 
 *   ThresholdedRelu(x)     - x if x >= alpha else 0
 * 
 *   ScaledTanh(x)          - alpha*Tanh(beta*x)
 * 
 *   HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
 * 
 *   Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
 * 
 *   Softsign(x)            - x/(1 + |x|)
 * 
 *   Softplus(x)            - log(1 + e^x)
 * 
 * Equations (Default: f=Sigmoid, g=Tanh):
 * 
 *   - zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)
 * 
 *   - rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)
 * 
 *   - ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset = 0
 * 
 *   - ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when linear_before_reset != 0
 * 
 *   - Ht = (1 - zt) (.) ht + zt (.) Ht-1
 * This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
 * 
 * Constraint T:
 *   Constrain input and output types to float tensors.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * 
 * Constraint T1:
 *   Constrain seq_lens to integer tensor.
 *   Allowed Types: tensor_int32
 * Input T X:
 *   The input sequences packed (and potentially padded) into one 3-D tensor
 *   with the shape of `[seq_length, batch_size, input_size]`.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * 
 * Input T W:
 *   The weight tensor for the gates. Concatenation of `W[zrh]` and `WB[zrh]`
 *   (if bidirectional) along dimension 0. This tensor has shape
 *   `[num_directions, 3*hidden_size, input_size]`.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * 
 * Input T R:
 *   The recurrence weight tensor. Concatenation of `R[zrh]` and `RB[zrh]` (if
 *   bidirectional) along dimension 0. This tensor has shape `[num_directions,
 *   3*hidden_size, hidden_size]`.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * 
 * Input T B:
 *   The bias tensor for the gates. Concatenation of `[Wb[zrh], Rb[zrh]]` and
 *   `[WBb[zrh], RBb[zrh]]` (if bidirectional) along dimension 0. This tensor
 *   has shape `[num_directions, 6*hidden_size]`. Optional: If not specified -
 *   assumed to be 0
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * 
 * Input T1 sequence_lens:
 *   Optional tensor specifying lengths of the sequences in a batch. If not
 *   specified - assumed all sequences in the batch to have length
 *   `seq_length`. It has shape `[batch_size]`.
 *   Allowed Types: tensor_int32
 * 
 * Input T initial_h:
 *   Optional initial value of the hidden. If not specified - assumed to be 0.
 *   It has shape `[num_directions, batch_size, hidden_size]`.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Output T Y:
 *   A tensor that concats all the intermediate output values of the hidden.
 *   It has shape `[seq_length, num_directions, batch_size, hidden_size]`.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * 
 * Output T Y_h:
 *   The last output value of the hidden. It has shape `[num_directions,
 *   batch_size, hidden_size]`.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Attribute FLOATS activation_alpha (optional):
 *   Optional scaling values used by some activation functions. The values are
 *   consumed in the order of activation functions, for example (f, g, h) in
 *   LSTM. Default values are the same as of corresponding ONNX operators.For
 *   example with LeakyRelu, the default alpha is 0.01.
 * 
 * Attribute FLOATS activation_beta (optional):
 *   Optional scaling values used by some activation functions. The values are
 *   consumed in the order of activation functions, for example (f, g, h) in
 *   LSTM. Default values are the same as of corresponding ONNX operators.
 * 
 * Attribute STRINGS activations (optional):
 *   A list of 2 (or 4 if bidirectional) activation functions for update,
 *   reset, and hidden gates. The activation functions must be one of the
 *   activation functions specified above. Optional: See the equations for
 *   default if not specified.
 * 
 * Attribute FLOAT clip (optional):
 *   Cell clip threshold. Clipping bounds the elements of a tensor in the
 *   range of [-threshold, +threshold] and is applied to the input of
 *   activations. No clip if not specified.
 * 
 * Attribute STRING direction (optional):
 *   Specify if the RNN is forward, reverse, or bidirectional. Must be one of
 *   forward (default), reverse, or bidirectional.
 * 
 * Attribute INT hidden_size (optional):
 *   Number of neurons in the hidden layer
 * 
 * Attribute INT linear_before_reset (optional):
 *   When computing the output of the hidden gate, apply the linear
 *   transformation before multiplying by the output of the reset gate.
 *
 * @since version 7
 *
 * @see github/workspace/onnx/defs/rnn/old.cc:1516
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#GRU
 */

operator_status
prepare_operator__ai_onnx__gru__7(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__gru__7;

typedef struct {
// no attributes
} context_operator__ai_onnx__gru__7;

operator_executer
resolve_operator__ai_onnx__gru__7(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gru__7(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gru__7__T_tensor_double__T1_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gru__7__T_tensor_float__T1_tensor_int32(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__gru__7__T_tensor_float16__T1_tensor_int32(
    node_context *ctx
);

# endif